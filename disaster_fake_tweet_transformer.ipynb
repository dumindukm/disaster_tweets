{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "disaster_fake_tweet_transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0EUF_HfMloi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ca48832-76bb-4129-aa23-9b31588c1cb4"
      },
      "source": [
        "!pip3 install pyspellchecker\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "import re\n",
        "from spellchecker import SpellChecker\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize  \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english')) \n",
        "porter = PorterStemmer()\n",
        "\n",
        "spell = SpellChecker()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.6/dist-packages (0.5.5)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfdvlD_PNCwI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd5718e-f378-452c-f949-abebd94280fc"
      },
      "source": [
        "def remove_html(text):\n",
        "    html = re.compile(r'<.*?>')\n",
        "    return html.sub(r'', text)\n",
        "\n",
        "def correct_spellings(text):\n",
        "    corrected_text = []\n",
        "    misspelled_words = spell.unknown(text.split())\n",
        "    for word in text.split():\n",
        "        if word in misspelled_words:\n",
        "            corrected_text.append(spell.correction(word))\n",
        "        else:\n",
        "            corrected_text.append(word)\n",
        "    print(corrected_text)\n",
        "    return \" \".join(corrected_text)\n",
        "\n",
        "def remove_stop_words(text): \n",
        "  word_tokens = word_tokenize(text) \n",
        "  \n",
        "  filtered_sentence = [porter.stem(w) for w in word_tokens if not w in stop_words] \n",
        "\n",
        "  return \" \".join(filtered_sentence)\n",
        "\n",
        "def text_clean(text):\n",
        "    text = text.lower()\n",
        "    text = remove_stop_words(text)\n",
        "    text = remove_URL(text)\n",
        "    text = remove_html(text)\n",
        "    text = remove_emoji(text)\n",
        "    text = remove_punct(text)\n",
        "    text = re.sub(r'\\n',' ', text)\n",
        "    text = re.sub('\\s+', ' ', text).strip()\n",
        "    text = correct_spellings(text)\n",
        "    return text\n",
        "  \n",
        "def remove_punct(text):\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(table)\n",
        "\n",
        "def remove_URL(text):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'', text)\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "def clean_data(ds):\n",
        "  ds['text'] = ds['new_text'].apply(lambda x: text_clean(x))\n",
        "  return ds\n",
        "\n",
        "def parallelize_dataframe(df, func, n_cores=4):\n",
        "    df_split = np.array_split(df, n_cores)\n",
        "    pool = Pool(n_cores)\n",
        "    df = pd.concat(pool.map(func, df_split))\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def get_data_for_model(data_file_path):\n",
        "    ds = pd.read_csv(data_file_path)\n",
        "    ds['location'].fillna(value='unk', inplace=True)\n",
        "    ds['keyword'].fillna(value='k_unk', inplace=True)\n",
        "    ds = ds.fillna('0')\n",
        "    ds.to_csv('train_clean.csv',index=False)\n",
        "    ds[\"new_text\"] = ds['text'] + ds['keyword'] + ds['location']\n",
        "    # ds['text'] = ds['new_text'].apply(lambda x: text_clean(x))\n",
        "    ds = parallelize_dataframe(ds,clean_data)\n",
        "    ds.to_csv('train_clean.csv',index=False)\n",
        "    X = ds[['text']] \n",
        "\n",
        "    Y = None\n",
        "    if 'target' in ds:\n",
        "        Y = ds[['target']]\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    le.fit(ds[[\"keyword\"]])\n",
        "    ds[\"keyword_encoder\"] = le.transform(ds[[\"keyword\"]])\n",
        "\n",
        "    # le = LabelEncoder()\n",
        "    le.fit(ds[[\"location\"]])\n",
        "    ds[\"location_encoder\"] = le.transform(ds[[\"location\"]])\n",
        "\n",
        "    return ds, X, Y\n",
        "\n",
        "ds, X, Y = get_data_for_model('train.csv')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['our', 'deeds', 'are', 'the', 'reason', 'of', 'this', 'earthquake', 'may', 'allah', 'forgive', 'us', 'allk_unkunk']\n",
            "['forest', 'fire', 'near', 'la', 'range', 'saska', 'canadak_unkunk']\n",
            "['all', 'residents', 'asked', 'to', 'shelter', 'in', 'place', 'are', 'being', 'notified', 'by', 'officers', 'no', 'other', 'evacuation', 'or', 'shelter', 'in', 'place', 'orders', 'are', 'expectedk_unkunk']\n",
            "['r3000', 'people', 'receive', 'wildfire', 'evacuation', 'orders', 'in', 'california', 'k_unkunk']\n",
            "['just', 'got', 'sent', 'this', 'photo', 'from', 'ruby', 'alaska', 'as', 'smoke', 'from', 'wildfire', 'pours', 'into', 'a', 'school', 'k_unkunk']\n",
            "['#rockyfire', 'update', 'of', 'california', 'hwyl', '20', 'closed', 'in', 'both', 'directions', 'due', 'to', 'lake', 'county', 'fire', '-', 'afire', '#wildfiresk_unkunk']\n",
            "['flood', 'disaster', 'heavy', 'rain', 'causes', 'flash', 'flooding', 'of', 'streets', 'in', 'manitou', 'colorado', 'springs', 'areask_unkunk']\n",
            "['ibm', 'on', 'top', 'of', 'the', 'hill', 'and', 'i', 'can', 'see', 'a', 'fire', 'in', 'the', 'woods...k_unkunk']\n",
            "[\"three's\", 'an', 'emergency', 'evacuation', 'happening', 'now', 'in', 'the', 'building', 'across', 'the', 'streetk_unkunk']\n",
            "['ibm', 'afraid', 'that', 'the', 'tornado', 'is', 'coming', 'to', 'our', 'area...k_unkunk']\n",
            "['three', 'people', 'died', 'from', 'the', 'heat', 'wave', 'so', 'fark_unkunk']\n",
            "['aha', 'south', 'tampa', 'is', 'getting', 'flooded', 'hah', 'wait', 'a', 'second', 'i', 'live', 'in', 'south', 'tampa', 'what', 'am', 'i', 'gonna', 'do', 'what', 'am', 'i', 'gonna', 'do', 'fuck', '#floodingk_unkunk']\n",
            "['training', 'flooding', 'florida', '#tampabay', 'tampa', '18', 'or', '19', 'days', 'ive', 'lost', 'count', 'k_unkunk']\n",
            "['flood', 'in', 'ago', 'myanmar', 'we', 'arrived', 'bagok_unkunk']\n",
            "['damage', 'to', 'school', 'bus', 'on', '80', 'in', 'multi', 'car', 'crash', 'breaking', 'k_unkunk']\n",
            "['whats', 'up', 'man?k_unkunk']\n",
            "['i', 'love', 'fruitsk_unkunk']\n",
            "['summer', 'is', 'lovelyk_unkunk']\n",
            "['my', 'car', 'is', 'so', 'fastk_unkunk']\n",
            "['what', 'a', 'goooooooaaaaaal!!!!!!k_unkunk']\n",
            "['this', 'is', 'ridiculous....k_unkunk']\n",
            "['london', 'is', 'cool', ';)k_unkunk']\n",
            "['love', 'skiingk_unkunk']\n",
            "['what', 'a', 'wonderful', 'day!k_unkunk']\n",
            "['loooooolk_unkunk']\n",
            "['no', 'way...i', \"can't\", 'eat', 'that', 'shitk_unkunk']\n",
            "['was', 'in', 'nec', 'last', 'week!k_unkunk']\n",
            "['love', 'my', 'girlfriendk_unkunk']\n",
            "['cool', ':)k_unkunk']\n",
            "['do', 'you', 'like', 'pasta?k_unkunk']\n",
            "['the', 'end!k_unkunk']\n",
            "['@bbcmtd', 'wholesale', 'markets', 'ablaze', 'http://t.co/lhyxeohy6cablazebirmingham']\n",
            "['we', 'always', 'try', 'to', 'bring', 'the', 'heavy', 'metal', 'art', 'http://t.co/yao1e0xngwablazeest.', 'september', '2012', '-', 'bristol']\n",
            "['#africanbaze:', 'breaking', 'news:nigeria', 'flag', 'set', 'ablaze', 'in', 'aba', 'http://t.co/2nndbgwyeiablazeafrica']\n",
            "['crying', 'out', 'for', 'more', 'set', 'me', 'ablazeablazephiladelphia,', 'pa']\n",
            "['on', 'plus', 'side', 'look', 'at', 'the', 'sky', 'last', 'night', 'it', 'was', 'ablaze', 'http://t.co/qqsmshaj3nablazelondon,', 'uk']\n",
            "['@phdsquares', 'mufc', \"they've\", 'built', 'so', 'much', 'hype', 'around', 'new', 'acquisitions', 'but', 'i', 'doubt', 'they', 'will', 'set', 'the', 'epl', 'ablaze', 'this', 'season.ablazepretoria']\n",
            "['inc', 'office', 'in', 'asia', 'set', 'ablaze', '-', 'http://t.co/3imaomknnaablazeworld', 'wide']\n",
            "['barbados', 'bridgetown', 'jamaica', '\\x89ûò', 'two', 'cars', 'set', 'ablaze', 'santa', 'cruz', '\\x89ûó', 'head', 'of', 'the', 'st', 'elizabeth', 'police', 'superintende...', 'http://t.co/wdueaj8q4jablazeunk']\n",
            "['ablaze', 'for', 'you', 'lord', ':dablazeparanaque', 'city']\n",
            "['check', 'these', 'out', 'http://t.co/roi2nsmejj', 'http://t.co/3tj8zjin21', 'http://t.co/yduixefipe', 'http://t.co/lxtjc87kls', '#nsfwablazelive', 'on', 'welham']\n",
            "['on', 'the', 'outside', \"you'se\", 'ablaze', 'and', 'alive', 'but', \"you'se\", 'dead', 'insideablazeunk']\n",
            "['had', 'an', 'awesome', 'time', 'visiting', 'the', 'cfc', 'head', 'office', 'the', 'anc', 'site', 'and', 'ablaze', 'thanks', 'to', 'tita', 'vida', 'for', 'taking', 'care', 'of', 'us', '??ablazemilky', 'way']\n",
            "['soon', 'pumped', 'for', 'ablaze', '????', '@southridgelifeablazeunk']\n",
            "['i', 'wanted', 'to', 'set', 'chicago', 'ablaze', 'with', 'my', 'preaching...', 'but', 'not', 'my', 'hotel', 'http://t.co/o9qknbfofxablazeunk']\n",
            "['i', 'gained', '3', 'followers', 'in', 'the', 'last', 'week', 'you', 'know', 'your', 'stats', 'and', 'grow', 'with', 'http://t.co/tiyulif5c6ablazeunk']\n",
            "['how', 'the', 'west', 'was', 'burned', 'thousands', 'of', 'wildfire', 'ablaze', 'in', 'california', 'alone', 'http://t.co/vl5tbr3wbrablazegreensboro,north', 'carolina']\n",
            "['building', 'the', 'perfect', 'blacklist', 'to', 'life', 'leave', 'the', 'streets', 'ablazeablazeunk']\n",
            "['check', 'these', 'out', 'http://t.co/roi2nsmejj', 'http://t.co/3tj8zjin21', 'http://t.co/yduixefipe', 'http://t.co/lxtjc87kls', '#nsfwablazelive', 'on', 'welham']\n",
            "['first', 'night', 'with', 'retainers', 'in.', 'its', 'quite', 'weird', 'better', 'get', 'used', 'to', 'it', 'i', 'have', 'to', 'wear', 'them', 'every', 'single', 'night', 'for', 'the', 'next', 'year', 'at', 'least.ablazeengland.']\n",
            "['deputies', 'man', 'shot', 'before', 'brighton', 'home', 'set', 'ablaze', 'http://t.co/gwnrhmso8kablazesheffield', 'township', 'ohio']\n",
            "['man', 'wife', 'get', 'six', 'years', 'jail', 'for', 'setting', 'ablaze', 'niece', 'http://t.co/ev1ahouczaablazeindia']\n",
            "['santa', 'cruz', '\\x89ûó', 'head', 'of', 'the', 'st', 'elizabeth', 'police', 'superintendent', 'linford', 'salmon', 'has', 'r', 'p.', '-', 'http://t.co/vplr5hka2u', 'http://t.co/sxhw2tnnlfablazebarbados']\n",
            "['police', 'arsonist', 'deliberately', 'set', 'black', 'church', 'in', 'north', 'carolinaåêablaze', 'http://t.co/pcxarbh9anablazeanaheim']\n",
            "['niches', 'el-bestia', \"'@alexis_sanchez:\", 'happy', 'to', 'see', 'my', 'teammates', 'and', 'training', 'hard', 'of', 'goodnight', 'gunners.??????', \"http://t.co/uc4j4jhvgr'ablazeabuja\"]\n",
            "['kurds', 'trampling', 'on', 'turkmen', 'flag', 'later', 'set', 'it', 'ablaze', 'while', 'others', 'vandalized', 'offices', 'of', 'turkmen', 'front', 'in', '#diyala', 'http://t.co/4izfdyc3cgablazeusa']\n",
            "['truck', 'ablaze', ':', 'r21', 'voortrekker', 'av.', 'outside', 'or', 'tambo', 'incl.', 'cargo', 'section', 'http://t.co/8kscqkfkkfablazesouth', 'africa']\n",
            "['set', 'our', 'hearts', 'ablaze', 'and', 'every', 'city', 'was', 'a', 'gift', 'and', 'every', 'skyline', 'was', 'like', 'a', 'kiss', 'upon', 'the', 'lips', '@\\x89û_', 'https://t.co/cyompz1a0zablazesao', 'paulo', 'brazil']\n",
            "['they', 'sky', 'was', 'ablaze', 'tonight', 'in', 'los', 'angeles', 'ibm', 'expecting', 'ig', 'and', 'fb', 'to', 'be', 'filled', 'with', 'sunset', 'shots', 'if', 'i', 'know', 'my', 'peeps!!ablazehollywoodland']\n",
            "['how', 'the', 'west', 'was', 'burned', 'thousands', 'of', 'wildfire', 'ablaze', 'in', 'california', 'alone', 'http://t.co/icsjgz9te1', 'climate', 'energy', 'http://t.co/9fxmn0l0bdablazeedmonton,', 'alberta', '-', 'treaty', '6']\n",
            "['revel', 'in', 'yours', 'wiv', 'videos', 'by', 'means', 'of', 'mac', 'farewell', 'ablaze', 'wiv', 'en', 'route', 'to', 'did', 'gtxrwmablazeunk']\n",
            "['progressive', 'greetings', 'in', 'about', 'a', 'month', 'students', 'would', 'have', 'set', 'their', 'pens', 'ablaze', 'in', 'the', 'torch', \"publications'...\", 'http://t.co/9fxpixqujtablazeinang', 'pamantasan']\n",
            "['rene', 'ablaze', 'camp', 'jacinto', '-', 'secret', 'k13', 'fallen', 'skies', 'edit', '-', 'mar', '30', '2013', 'https://t.co/7mlmsuzv1zablazetwitter', 'lockout', 'in', 'progress']\n",
            "['@navista7', 'steve', 'these', 'fires', 'out', 'here', 'are', 'something', 'else', 'california', 'is', 'a', 'tinderbox', '-', 'and', 'this', 'clown', 'was', 'setting', 'my', 'hood', 'ablaze', '@news24680ablazeconcord,', 'ca']\n",
            "['#nowplaying:', 'rene', 'ablaze', 'camp', 'ian', 'buff', '-', 'magnitude', 'http://t.co/av2jsjfftc', '#edmablazecalgary,', 'ab']\n",
            "['@nxwestmidlands', 'huge', 'fire', 'at', 'wholesale', 'markets', 'ablaze', 'http://t.co/rwzbfvnxerablazebirmingham']\n",
            "['ablaze', 'what', 'time', 'does', 'your', 'talk', 'go', 'until', 'i', \"don't\", 'know', 'if', 'i', 'can', 'make', 'it', 'due', 'to', 'work.ablazesan', 'francisco']\n",
            "['i', \"can't\", 'have', 'kids', 'cut', 'i', 'got', 'in', 'a', 'bicycle', 'accident', 'camp', 'split', 'my', 'testicles', 'its', 'impossible', 'for', 'me', 'to', 'have', 'kids', 'michael', 'you', 'are', 'the', 'fatheraccidentclvlnd']\n",
            "['accident', 'on', 'g-24', 'w', '#nashvilletraffic.', 'traffic', 'moving', 'mm', 'slower', 'than', 'usual', 'https://t.co/0ghk693egjaccidentnashville,', 'tn']\n",
            "['accident', 'center', 'lane', 'blocked', 'in', '#santaclara', 'on', 's-100', 'nb', 'before', 'great', 'america', 'pay', '#bayarea', 'traffic', 'http://t.co/pmlohzurwraccidentsanta', 'clara', 'ca']\n",
            "['http://t.co/gkye6gjtk5', 'had', 'a', '#personalinjury', 'accident', 'this', 'summer', 'read', 'our', 'advice', 'camp', 'see', 'how', 'a', 'solicitor', 'can', 'help', '#otleyhouraccidentuk']\n",
            "['#stlouis', '#caraccidentlawyer', 'speeding', 'among', 'top', 'causes', 'of', 'teen', 'accidents', 'https://t.co/k4zomof319', 'https://t.co/s2kxvm0cba', 'car', 'accident', 'tee\\x89û_accidentst.', 'louis', 'mo']\n",
            "['reported', 'motor', 'vehicle', 'accident', 'in', 'curry', 'on', 'herman', 'rd', 'near', 'stephenson', 'involving', 'an', 'overturned', 'vehicle', 'please', 'use...', 'http://t.co/ybjezkurw1accidentwalker', 'county', 'alabama']\n",
            "['bigrigradio', 'live', 'accident', 'awarenessaccidentaustralia']\n",
            "['g-7', 'mile', 'marker', '31', 'south', 'mooresville', 'tredell', 'vehicle', 'accident', 'ramp', 'closed', 'at', 'mi6', 's.18', 'pmaccidentnorth', 'carolina']\n",
            "['rt', '@sleepjunkies:', 'sleeping', 'pills', 'double', 'your', 'risk', 'of', 'a', 'car', 'accident', 'http://t.co/7s9nm1fictaccidentunk']\n",
            "['by', 'accident', 'they', 'knew', 'what', 'was', 'gon', 'happen', 'https://t.co/ysxun5vcehaccidentnorf', 'carolina']\n",
            "['traffic', 'accident', 'n', 'carrillo', 'hwy/magellan', 'av', 'mir', '(08/06/15', '11:03:58)accidentsan', 'mateo', 'county', 'ca']\n",
            "['g-7', 'mile', 'marker', '31', 'to', '40', 'south', 'mooresville', 'tredell', 'vehicle', 'accident', 'congestion', 'at', 'mi6', 's.18', 'pmaccidentnorth', 'carolina']\n",
            "['the', 'pastor', 'was', 'not', 'in', 'the', 'scene', 'of', 'the', 'accident......who', 'was', 'the', 'owner', 'of', 'the', 'range', 'rover', '?accidentnjoro,', 'kenya']\n",
            "['mom', 'we', \"didn't\", 'get', 'home', 'as', 'fast', 'as', 'we', 'wished', 'me', 'why', 'is', \"that'\", 'mom', 'there', 'was', 'an', 'accident', 'and', 'some', 'truck', 'spilt', 'mayonnaise', 'all', 'over', '??????accidentunk']\n",
            "['i', 'was', 'in', 'a', 'horrible', 'car', 'accident', 'this', 'past', 'sunday', 'ibm', 'finally', 'able', 'to', 'get', 'around', 'thank', 'you', 'god??accidentyour', 'sisters', 'bedroom']\n",
            "['can', 'wait', 'to', 'see', 'how', 'pissed', 'donnie', 'is', 'when', 'i', 'tell', 'him', 'i', 'was', 'in', 'another', 'accident??accidentunk']\n",
            "['#truckcrash', 'overturns', 'on', '#fortworth', 'interstate', 'http://t.co/rs22lj4qfp', 'click', 'here', 'if', \"you'se\", 'been', 'in', 'a', 'crash&gt;http://t.co/ld0uniyw4kaccidentarlington,', 'tx']\n",
            "['accident', 'in', 'nashville', 'on', 'us', '23', 'sb', 'before', 'sr', '752', 'traffic', 'http://t.co/hylmo0wgfiaccidentsouth', 'bloomfield', 'oh']\n",
            "['carolina', 'accident', 'motorcyclist', 'dies', 'in', 'p-40', 'crash', 'with', 'car', 'that', 'crossed', 'median', 'a', 'motorcycle', 'rider', 'traveling...', 'http://t.co/p18lzrlmy6accidentunk']\n",
            "['fbi', 'cad:fyi:', 'accident', 'property', 'damage;nhs;999', 'pine', 'rd/horndale', 'draccidentnew', 'hanover', 'county', 'nc']\n",
            "['rt', 'naafi', 'first', 'accident', 'in', 'years', 'turning', 'onto', 'chandanee', 'mag', 'from', 'near', 'm.a.', 'taxi', 'rammed', 'into', 'me', 'while', 'i', 'was', 'halfway', 'turned', 'everyone', 'conf\\x89û_accidentmaldives']\n",
            "['accident', 'left', 'lane', 'blocked', 'in', 'manchester', 'on', 'rt', '293', 'nb', 'before', 'eddy', 'rd', 'stop', 'and', 'go', 'traffic', 'back', 'to', 'ah-ha', 'delay', 'of', '4', 'mins', '#trafficaccidentmanchester,', 'nh']\n",
            "['accident', 'property', 'damage', 'pine', 'rd/horndale', 'draccidentwilmington,', 'nc']\n",
            "['????', 'it', 'was', 'an', 'accident', 'http://t.co/oia5fxi4gmaccidentunk']\n",
            "['fbi', 'cad:fyi:', 'accident', 'property', 'damage;wpd;1600', 's', 'with', 'staccidentnew', 'hanover', 'county', 'nc']\n",
            "['8/6/2015@2:09', 'pm', 'traffic', 'accident', 'no', 'injury', 'at', '2781', 'willis', 'foreman', 'rd', 'http://t.co/vckit6edevaccidentunk']\n",
            "['aashiqui', 'actress', 'and', 'aggarwal', 'on', 'her', 'near-fatal', 'accident', 'http://t.co/6otfp31lqwaccidentglobal']\n",
            "['suffield', 'alberta', 'accident', 'https://t.co/bptmlf4p10accidentalberta', '|', 'saska', '|', 'montana']\n",
            "['9', 'mile', 'backup', 'on', 'g-7', 'south...accident', 'blocking', 'the', 'right', '2', 'lanes', 'at', 'exit', '31', 'langtry', 'rd...consider', 'nc', '115', 'or', 'nc', '150', 'to', 'nc', '16', 'as', 'alternateaccidentcharlotte']\n",
            "['has', 'an', 'accident', 'changed', 'your', 'life', 'we', 'will', 'help', 'you', 'determine', 'options', 'that', 'can', 'financially', 'support', 'life', 'care', 'plans', 'and', 'on-going', 'treatment.accidentbaton', 'rouge', 'la']\n",
            "['breaking', 'there', 'was', 'a', 'deadly', 'motorcycle', 'car', 'accident', 'that', 'happened', 'to', '#hagerstown', 'today', 'ill', 'have', 'more', 'details', 'at', '5', '@your4state.', '#whagaccidenthagerstown,', 'md']\n",
            "['@flowri', 'were', 'you', 'marinading', 'it', 'or', 'was', 'it', 'an', 'accident?accidentgloucestershire', ',', 'uk']\n",
            "['only', 'had', 'a', 'car', 'for', 'not', 'even', 'a', 'week', 'and', 'got', 'in', 'a', 'fucking', 'car', 'accident', 'p.', 'mfs', \"can't\", 'fucking', 'drive', '.accidentunk']\n",
            "['.@norwaymfa', 'bahrain', 'police', 'had', 'previously', 'died', 'in', 'a', 'road', 'accident', 'they', 'were', 'not', 'killed', 'by', 'explosion', 'https://t.co/gfjfgtodadaccidentuk']\n",
            "['i', 'still', 'have', 'not', 'heard', 'church', 'leaders', 'of', 'kenya', 'coming', 'forward', 'to', 'comment', 'on', 'the', 'accident', 'issue', 'and', 'disciplinary', 'measures#arrestpastorngangaaccidentnairobi,', 'kenya']\n",
            "['@aftershock_delo', 'scum', 'ps', 'live', 'and', 'the', 'game...', 'cyaaftershockinstagram', '-', '@heyimginog']\n",
            "['the', 'man', 'who', 'can', 'drive', 'himself', 'further', 'once', 'the', 'effort', 'gets', 'painful', 'is', 'the', 'man', 'who', 'will', 'win', 'roger', 'bannisteraftershock304']\n",
            "['320', 'air', 'icemoon', 'aftershocks', '|', 'http://t.co/ynxnvvkcda', '|', '@djicemoon', '|', '#dubstep', '#trapmusic', 'dnb', 'erm', 'dance', '#ices\\x89û_', 'http://t.co/weqpesenkuaftershockswitzerland']\n",
            "['there', 'is', 'no', 'victory', 'at', 'bargain', 'basement', 'prices', 'dwight', 'david', 'eisenhoweraftershock304']\n",
            "['320', 'air', 'icemoon', 'aftershocks', '|', 'http://t.co/vam5podgyw', '|', '@djicemoon', '|', '#dubstep', '#trapmusic', 'dnb', 'erm', 'dance', '#ices\\x89û_', 'http://t.co/zevakjapczaftershockus']\n",
            "['nobody', 'remembers', 'who', 'came', 'in', 'second', 'charles', 'schulzaftershock304']\n",
            "['@aftershock_delo', 'im', 'speaking', 'from', 'someone', 'that', 'is', 'using', 'a', 'scum', 'on', 'b1', 'most', 'of', 'them', 'people', 'will', 'end', 'up', 'getting', 'on', 'for', 'ps', 'also.aftershockinstagram', '-', '@heyimginog']\n",
            "['the', 'harder', 'the', 'conflict', 'the', 'more', 'glorious', 'the', 'triumph', 'thomas', 'paineaftershock304']\n",
            "['#growingupspoiled', 'going', 'clay', 'pigeon', 'shooting', 'and', 'crying', 'because', 'of', 'the', \"'aftershock'aftershockunk\"]\n",
            "['so', 'i', 'guess', 'no', 'one', 'actually', 'wants', 'any', 'free', 'aftershocks', 'tc.....aftershocksomewhere', 'only', 'we', 'know', '?']\n",
            "['aftershocks', 'was', 'the', 'most', 'terrifying', 'best', 'roller', 'coaster', 'ive', 'ever', 'been', 'on', 'disclaimer', 'ive', 'been', 'on', 'very', 'few.aftershockunk']\n",
            "['aftershocks', 'https://t.co/xmwodfmtuiaftershockbelgium']\n",
            "['320', 'air', 'icemoon', 'aftershocks', '|', 'http://t.co/m4jdzmgjow', '|', '@djicemoon', '|', '#dubstep', '#trapmusic', 'dnb', 'erm', 'dance', '#ices\\x89û_', 'http://t.co/n0uhasfkbvaftershockswitzerland']\n",
            "['320', 'air', 'icemoon', 'aftershocks', '|', 'http://t.co/vam5podgyw', '|', '@djicemoon', '|', '#dubstep', '#trapmusic', 'dnb', 'erm', 'dance', '#ices\\x89û_', 'http://t.co/zevakjapczaftershockus']\n",
            "['320', 'air', 'icemoon', 'aftershocks', '|', 'http://t.co/e14epzhoth', '|', '@djicemoon', '|', '#dubstep', '#trapmusic', 'dnb', 'erm', 'dance', '#ices\\x89û_', 'http://t.co/22a9d5do6qaftershockunk']\n",
            "['@kjfordays', 'ibm', 'seeing', 'them', 'and', 'issues', 'at', 'aftershocks', '??aftershockdope', 'show']\n",
            "['320', 'air', 'icemoon', 'aftershocks', '|', 'http://t.co/thyzomvwu0', '|', '@djicemoon', '|', '#dubstep', '#trapmusic', 'dnb', 'erm', 'dance', '#ices\\x89û_', 'http://t.co/83joo0xk29aftershockswitzerland']\n",
            "['320', 'air', 'icemoon', 'aftershocks', '|', 'http://t.co/thyzomvwu0', '|', '@djicemoon', '|', '#dubstep', '#trapmusic', 'dnb', 'erm', 'dance', '#ices\\x89û_', 'http://t.co/83joo0xk29aftershockswitzerland']\n",
            "['#wisdomwed', 'bonus', '-', '5', 'minute', 'daily', 'habits', 'that', 'could', 'really', 'improve', 'your', 'life', 'how', 'many', 'do', 'you', 'already', 'do', '#lifehacks', 'http://t.co/tbm9fqb8cwaftershockoshawa,', 'canada']\n",
            "['aftershocks', 'protect', 'yourself', 'and', 'profit', 'in', 'the', 'next', 'global', 'financial', 'meltdown', 'by', 'david', 'windeler', 'ttp', 'http://t.co/wztz4hgmvqaftershockbaker', 'city', 'oregon']\n",
            "['that', 'moment', 'when', 'you', 'get', 'on', 'a', 'scary', 'roller', 'coaster', 'and', 'the', 'guy', 'behind', 'you', 'is', 'just', 'screaming', 'bloody', 'murder', '??????', '#silverwood', '#aftershockaftershockunk']\n",
            "['aftershocks', '\\x89ã¢', '(2010)', 'full\\x89ã¢', 'streaming', '-', 'couture', 'http://t.co/vve3usesgfaftershockunk']\n",
            "['&gt;&gt;', 'b15', 'aftershocks', ':', 'protect', 'yourself', 'and', 'profit', 'in', 'the', 'next', 'global', 'financial...', 'book', 'http://t.co/f6ntuc734z', '@esquireattireaftershockunited', 'states']\n",
            "['sometimes', 'you', 'face', 'difficulties', 'not', 'because', \"you'se\", 'doing', 'something', 'wrong', 'but', 'because', \"you'se\", 'doing', 'something', 'right', '-', 'joel', 'osteenaftershock304']\n",
            "['the', 'only', 'thing', 'that', 'stands', 'between', 'you', 'and', 'your', 'dream', 'is', 'the', 'will', 'to', 'try', 'and', 'the', 'belief', 'that', 'it', 'is', 'actually', 'possible', '-', 'joel', 'brownaftershock304']\n",
            "['praise', 'god', 'that', 'we', 'have', 'ministry', 'that', 'tells', 'it', 'like', 'it', 'is!!!', 'now', '#wdyouth', '#biblestudy', 'https://t.co/ujk0e5gbccaftershockmarysville', 'ca']\n",
            "['remembering', 'that', 'you', 'are', 'going', 'to', 'die', 'is', 'the', 'best', 'way', 'i', 'know', 'to', 'avoid', 'the', 'trap', 'of', 'thinking', 'you', 'have', 'something', 'to', 'lose', '\\x89ûò', 'steve', 'jobsaftershock304']\n",
            "['tried', 'orange', 'aftershocks', 'today', 'my', 'life', 'will', 'never', 'be', 'the', 'sameaftershockhermosa', 'beach', 'ca']\n",
            "['@onfireanders', 'i', 'love', 'you', 'bbaftershockunk']\n",
            "['aftershocks', 'https://t.co/jv8ppkhjy7aftershockunk']\n",
            "['aftershocks', 'back', 'to', 'school', 'kick', 'off', 'was', 'great', 'i', 'want', 'to', 'thank', 'everyone', 'for', 'making', 'it', 'possible', 'what', 'a', 'great', 'night.aftershockunk']\n",
            "['people', 'who', 'say', 'it', 'cannon', 'be', 'done', 'should', 'not', 'interrupt', 'those', 'who', 'are', 'doing', 'it.', '\\x89ûò', 'george', 'bernard', 'shawaftershock304']\n",
            "['the', 'first', 'man', 'gets', 'the', 'oyster', 'the', 'second', 'man', 'gets', 'the', 'shell', 'andrew', 'carnegieaftershock304']\n",
            "['anyone', 'need', 'a', 'pou', 'tonight', 'i', 'play', 'hybrid', 'slayer', 'ps', 'e.', 'hm', '@cod8sandscrims', '@empirikgaming', '@codawscrims', '@4tp_kotc', '@4tpfa', '@aftershock_orgaftershockunk']\n",
            "['experts', 'in', 'france', 'begin', 'examining', 'airplane', 'debris', 'found', 'on', 'reunion', 'island', 'french', 'air', 'accident', 'experts', 'o.k.', 'http://t.co/yvvpznzmxg', '#newsairplane%20accident19.600858,', '-99.047821']\n",
            "['strict', 'liability', 'in', 'the', 'context', 'of', 'an', 'airplane', 'accident', 'pilot', 'error', 'is', 'a', 'common', 'component', 'of', 'most', 'aviation', 'c.a.', 'http://t.co/6cz3bohrd4airplane%20accidentpennsylvania']\n",
            "['@crobscarla', 'your', 'lifetime', 'odds', 'of', 'dying', 'from', 'an', 'airplane', 'accident', 'are', '1', 'in', '8015.airplane%20accidentsalt', 'lake', 'city', 'utah']\n",
            "['experts', 'in', 'france', 'begin', 'examining', 'airplane', 'debris', 'found', 'on', 'reunion', 'island', 'french', 'air', 'accident', 'experts', 'on', 'wedn...', 'http://t.co/bkpfpogysiairplane%20accidentpalo', 'alton', 'ca']\n",
            "['@alexalltimelow', 'aww', 'there', 'on', 'an', 'airplane', 'accident', 'and', 'there', 'gonna', 'die', 'what', 'a', 'cities', '????', 'good', 'job!airplane%20accidentunk']\n",
            "['family', 'members', 'of', 'osaka', 'bin', 'laden', 'have', 'died', 'in', 'an', 'airplane', 'accident', 'how', 'ironic', '??????', 'mmmm', 'gov', 'shit', 'i', 'suspectairplane%20accidentspain', 'but', 'opa-locka,', 'fl']\n",
            "['man', 'goes', 'into', 'airplane', 'engine', 'accident', 'http://t.co/tyjxrfd3st', 'via', '@youtubeairplane%20accidentjaipur,', 'india']\n",
            "['horrible', 'accident', 'man', 'died', 'in', 'wings', 'of', 'airplane', '(29-07-2015)', 'http://t.co/i7kztevb2vairplane%20accidenthyderabad', 'telangana', 'india']\n",
            "['a', 'cessna', 'airplane', 'accident', 'in', 'ocampo', 'coahuila', 'mexico', 'on', 'july', '29', '2015', 'killed', 'four', 'men', 'including', 'a', 'state', 'of', 'coahuila', 'government', 'official.airplane%20accidenteagle', 'pass', 'texas']\n",
            "['horrible', 'accident', 'man', 'died', 'in', 'wings', 'airplane', '(29-07-2015)', '#watchthevideo', 'http://t.co/p64xrvgjikairplane%20accidentbangalore']\n",
            "['experts', 'in', 'france', 'begin', 'examining', 'airplane', 'debris', 'found', 'on', 'reunion', 'island', 'http://t.co/lsmx2vwr3j', 'french', 'air', 'accident', 'experts', 'on', 'wednesday\\x89û_airplane%20accidentfinancial', 'news', 'and', 'views']\n",
            "['experts', 'in', 'france', 'begin', 'examining', 'airplane', 'debris', 'found', 'on', 'reunion', 'island', 'french', 'air', 'accident', 'experts', 'on', 'wednesday', 'began', 'examining', 't...airplane%20accidentunk']\n",
            "['ca', '#votejkt48id', 'mbataweel:', 'trip', '#binladen', 'family', 'members', 'who', 'killed', 'in', 'an', 'airplanes', 'accidentairplane%20accidentindonesia']\n",
            "['i', 'almost', 'sent', 'my', 'coworkers', 'nudes', 'on', 'accident', 'thank', 'god', 'for', 'airplane', 'modeairplane%20accidenty(our)', 'boyfriends', 'legs']\n",
            "['@mickinyman', '@theatlantic', 'that', 'or', 'they', 'might', 'be', 'killed', 'in', 'an', 'airplane', 'accident', 'in', 'the', 'night', 'a', 'car', 'wreck', 'politics', 'at', 'its', 'best.airplane%20accidentnew', 'mexico', 'usa']\n",
            "['experts', 'in', 'france', 'begin', 'examining', 'airplane', 'debris', 'found', 'on', 'reunion', 'island', 'french', 'air', 'accident', 'experts', 'o.k.', 'http://t.co/tagzbcxfj0', '#mlbairplane%20accidentsomewhere', 'out', 'there']\n",
            "['this', 'is', 'unbelievably', 'insane', 'man', 'airport', 'airplane', 'aircraft', 'aeroplane', 'runway', 'accident', '#freaky\\x89û_', 'https://t.co/cezhq7czllairplane%20accidentunk']\n",
            "['horrible', 'accident', '|', 'man', 'died', 'in', 'wings', 'of', 'airplaneåê(29-07-2015)', 'http://t.co/wq3wjsgphlairplane%20accidentmumbai', 'india']\n",
            "['horrible', 'accident', 'man', 'died', 'in', 'wings', 'of', 'airplane', '(29-07-2015)', 'http://t.co/tfcdronra6airplane%20accidentsri', 'lanka']\n",
            "['sama', 'bin', 'ladies', 'family', 'dead', 'in', 'airplane', 'crash', 'naturally', 'no', 'accident.airplane%20accidentnot', 'a', 'u.s', 'resident']\n",
            "['pilot', 'dies', 'in', 'plane', 'crash', 'at', 'car', 'festival', 'https://t.co/kq9ae6ap2b', 'via', '@youtube', 'crash', 'aircraft', 'airplane', 'pilot', 'death', 'accident', '#carfestairplane%20accidentunk']\n",
            "['strict', 'liability', 'in', 'the', 'context', 'of', 'an', 'airplane', 'accident', '-', 'http://t.co/gibyqhhkpkairplane%20accidentlehigh', 'valley', 'pa']\n",
            "['don', 'brazil', 'experts', 'in', 'france', 'begin', 'examining', 'airplane', 'debris', 'found', 'on', 'reunion', 'island', 'french', 'air', 'accident', 'exp...', 'http://t.co/m9ig3wq8lqairplane%20accidentcanada']\n",
            "['experts', 'in', 'france', 'begin', 'examining', 'airplane', 'debris', 'found', 'on', 'reunion', 'island', 'french', 'air', 'accident', 'experts', 'on', 'wedn...', 'http://t.co/v4smaeslk5airplane%20accidentunk']\n",
            "['horrible', 'accident', 'man', 'died', 'in', 'wings', 'of', '\\x89ûïairplane\\x89û\\x9d', '29-07-2015.', 'wwf', 'you', 'can\\x89ûªt', 'believe', 'your', 'eyes', '\\x89ûò...', 'http://t.co/6ffylajwpsairplane%20accidentthrissur']\n",
            "['+', 'nicole', 'fletcher', 'one', 'of', 'a', 'victim', 'of', 'crashed', 'airplane', 'few', 'times', 'ago', 'the', 'accident', 'left', 'a', 'little', 'bit', 'trauma', 'for', 'her', 'although', 'sheds', '+airplane%20accidenthavenford']\n",
            "['omg', 'horrible', 'accident', 'man', 'died', 'in', 'wings', 'of', 'airplane', 'http://t.co/xdxdprcpnsairplane%20accidentindia']\n",
            "['omg', 'i', \"don't\", 'believe', 'this', 'trip', 'bro', 'airplane', 'accident', '#jetengine', '#turbojet', 'boing', 'g60', 'http://t.co/kxxnszp6nkairplane%20accident92']\n",
            "['experts', 'in', 'france', 'begin', 'examining', 'airplane', 'debris', 'found', 'on', 'reunion', 'island', 'french', 'air', 'accident', 'experts', 'on', 'wednesday', 'began', 'examining', 't...airplane%20accidentunk']\n",
            "['i', 'had', 'a', 'airplane', 'accident.airplane%20accidentisrael']\n",
            "['my', 'phone', 'looks', 'like', 'it', 'was', 'in', 'a', 'car', 'ship', 'airplane', 'accident', 'terribleairplane%20accidentfashion', 'heaven', 'igg', 'tide']\n",
            "['statistically', 'ibm', 'at', 'more', 'of', 'risk', 'of', 'getting', 'killed', 'by', 'a', 'cop', 'than', 'i', 'am', 'of', 'dying', 'in', 'an', 'airplane', 'accident.airplane%20accidentsan', 'francisco', 'ca']\n",
            "['airplane', 'crashes', 'on', 'house', 'in', 'colombia', '12', 'people', 'die', 'in', 'accident', 'https://t.co/zhjlflbhzlairplane%20accidentitaly']\n",
            "['the', 'shooting', 'or', 'the', 'airplane', 'accident', 'https://t.co/iecc1jdoubairplane%20accidentnyc']\n",
            "['could', 'a', 'drone', 'cause', 'an', 'airplane', 'accident', 'pilots', 'worried', 'about', 'use', 'of', 'drones', 'est.', 'in', 'close', 'vicinity', 'of', 'airports', 'http://t.co/kz35rgngjf', '#airplane%20accidenttoronto']\n",
            "['early', 'wake', 'up', 'call', 'from', 'my', 'sister', 'begging', 'me', 'to', 'come', 'over', 'camp', 'ride', 'her', 'in', 'the', 'ambulance', 'to', 'the', 'hospital', '#rodkiaiambulanceunk']\n",
            "['http://t.co/ay6zzcupnz', 'twelve', 'feared', 'killed', 'in', 'pakistani', 'air', 'ambulance', 'helicopter', 'crash', 'http://t.co/sc9dns41mcambulancejackson']\n",
            "['two', 'air', 'ambulances', 'on', 'scene', 'of', 'serious', 'crash', 'between', 'two', 'cars', 'and', 'lorry', 'in', 'p.', '-', 'http://t.co/9pfeaqeski', 'http://t.co/fntg70rnkx', '|', '#emsne\\x89û_ambulancenew', 'york', '/', 'worldwide']\n",
            "['twelve', 'feared', 'killed', 'in', 'pakistani', 'air', 'ambulance', 'helicopter', 'crash', '-', 'reuters', 'http://t.co/mdnugvubwn', '#yugvaniambulanceunk']\n",
            "['leading', 'emergency', 'services', 'boss', 'welcomes', 'new', 'ambulance', 'charity', 'http://t.co/mj2jq2psv6ambulancenew', 'orleans', 'la']\n",
            "['anyone', 'travelling', 'aberystwyth-shrewsbury', 'right', 'now', \"three's\", 'been', 'an', 'incident', 'services', 'at', 'a', 'halt', 'just', 'outside', 'shrews', 'ambulance', 'on', 'scene.ambulancewest', 'wales']\n",
            "['twelve', 'feared', 'killed', 'in', 'pakistani', 'air', 'ambulance', 'helicopter', 'crash', 'http://t.co/xum8ylcb4qambulanceunk']\n",
            "['ambulance', 'sprinter', 'automatic', 'frontline', 'vehicle', 'choice', 'of', '14', 'let', 'compliant', '|', 'bay', 'http://t.co/4evttqpeiaambulancehappily', 'married', 'with', '2', 'kids']\n",
            "['new', 'nanotech', 'device', 'will', 'be', 'able', 'to', 'target', 'and', 'destroy', 'blood', 'clots', 'http://t.co/hfy5v3slbbambulancecambridge,', 'ma']\n",
            "['@20skyhawkmm20', '@traplord_29', '@fredosantana300', '@lilreese300', 'it', 'was', 'hella', 'crazy', '3', 'fights', 'an', 'ambulance', 'and', 'a', 'couple', 'most', 'pits', '??ambulancearizona']\n",
            "['if', 'i', 'get', 'run', 'over', 'by', 'an', 'ambulance', 'am', 'i', 'lucky', '#justsaying', '#randomthoughtambulancemumbai']\n",
            "['news', 'twelve', 'feared', 'killed', 'in', 'pakistani', 'air', 'ambulance', 'helicopter', 'crash', 'http://t.co/bfes5twbzt', '#til_now', '#dnaambulanceunk']\n",
            "['http://t.co/7xglah10zl', 'twelve', 'feared', 'killed', 'in', 'pakistani', 'air', 'ambulance', 'helicopter', 'crash', 'http://t.co/thmblaatzpambulanceamsterdam']\n",
            "['@tanslash', 'waiting', 'for', 'an', 'ambulanceambulanceswindon,england']\n",
            "['@fouseytube', 'you', 'ok', 'need', 'a', 'ambulance', 'hahahah', 'that', 'was', 'good', 'http://t.co/zsberqnn9nambulanceunk']\n",
            "['ambulance', 'sprinter', 'automatic', 'frontline', 'vehicle', 'choice', 'of', '14', 'let', 'compliant', '|', 'bay', 'http://t.co/q8ivrzojzvambulancehappily', 'married', 'with', '2', 'kids']\n",
            "['pakistan', 'air', 'ambulance', 'helicopter', 'crash', 'kills', 'nine', 'http://t.co/8e7ry8ebmfambulanceunk']\n",
            "['@thenissonian', '@rejectdcartoons', 'nissan', 'are', 'you', 'ok', 'do', 'you', 'need', 'medical', 'assistance', 'i', 'can', 'call', 'an', 'ambulance', 'if', 'you', 'need', 'me', 'toambulancewilliamstown,', 'vt']\n",
            "['ems', 'ny', 'eats', 'petition', 'for', 'g17', 'per', 'hour', '\\x89û÷minimum', 'wage\\x89ûª', 'http://t.co/4oa6swlxmr', 'gems', 'paramedics', '#ambulanceambulancenorth', 'carolina', 'usa']\n",
            "['http://t.co/fcqmkffflw', 'twelve', 'feared', 'killed', 'in', 'pakistani', 'air', 'ambulance', 'helicopter', 'crash', 'http://t.co/vayaymbnguambulanceunk']\n",
            "['twelve', 'feared', 'killed', 'in', 'pakistani', 'air', 'ambulance', 'helicopter', 'crash', 'http://t.co/3brme6sn4tambulancekarachi']\n",
            "['ambulance', 'sprinter', 'automatic', 'frontline', 'vehicle', 'choice', 'of', '14', 'let', 'compliant', '|', 'bay', 'http://t.co/ujrx9kgawpambulancehappily', 'married', 'with', '2', 'kids']\n",
            "['ambulance', 'sprinter', 'automatic', 'frontline', 'vehicle', 'choice', 'of', '14', 'let', 'compliant', '|', 'bay', 'http://t.co/kp2lf4auteambulancehappily', 'married', 'with', '2', 'kids']\n",
            "['@kiwi_karyn', 'check', 'out', 'whats', 'in', 'my', 'parking', 'lot', 'he', 'said', 'that', 'until', 'last', 'year', 'it', 'was', 'an', 'ambulance', 'in', 'st', 'johns', 'http://t.co/hpvodud7ipambulanceloveland', 'colorado']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVIAEaH2OD-H"
      },
      "source": [
        "def tokenize_tweets(ds):\n",
        "    word_tokens = []\n",
        "\n",
        "    for sentence in ds['text']:\n",
        "        word_tokens.append('<start> ' +sentence+ ' <end>')#'<start> ' + w + ' <end>'\n",
        "\n",
        "    return word_tokens\n",
        "\n",
        "def tokenize_target(ds):\n",
        "  word_tokens = []\n",
        "  for sentence in ds['target']:\n",
        "    if sentence == 0:\n",
        "      word_tokens.append('<start> ' +'negative'+ ' <end>')\n",
        "    else:\n",
        "      word_tokens.append('<start> ' +'positive'+ ' <end>')\n",
        "\n",
        "  return word_tokens\n",
        "\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "tokens = tokenize_tweets(ds)\n",
        "input_tensor, inp_lang_tokenizer = tokenize(tokens)\n",
        "targ_tokens=tokenize_target(ds)\n",
        "target_tensor, targ_lang_tokenizer = tokenize(targ_tokens)\n",
        "\n",
        "print(input_tensor[5])\n",
        "print(target_tensor[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o2UUgilPa2O"
      },
      "source": [
        "x_train ,x_test , y_train,y_test =  train_test_split(\n",
        "    input_tensor, target_tensor, test_size=0.1, random_state=42)\n",
        "\n",
        "# print(x_train)\n",
        "# print(y_train)\n",
        "# print(np.array(y_train))\n",
        "# max_length_targ = x_train.shape[1]\n",
        "# print(max_length_targ )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1_Lk4yOQ-Sg"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "\n",
        "convert(inp_lang_tokenizer ,  input_tensor[0])\n",
        "convert(targ_lang_tokenizer ,  target_tensor[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gML2eSc3xKwr"
      },
      "source": [
        "BUFFER_SIZE = len(x_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(x_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez8naKysxz66"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVInFSR9vdOj"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXSo4U6WyHqM"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPs3da63w4ww"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMKjS2tNz6qT"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_TRwpfZzLsT"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCp_efehz_Ip"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEuVCycWzauo"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_XwWDU4zeX9"
      },
      "source": [
        "import os\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1Xe6rplzkt3"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hULYFSjxzLhl"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOFAa4LZkMdN"
      },
      "source": [
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3b92EivgE9q"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  # sentence = preprocess_sentence(sentence)\n",
        "  \n",
        "  inputs = [inp_lang_tokenizer.word_index.get(i,0) for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  # print(inputs)\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
        "  predicted_ids=[]\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    predicted_ids.append(targ_lang_tokenizer.index_word[predicted_id])\n",
        "    result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot , predicted_ids\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot, predicted_ids\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJaZlJougIYR"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STQCg8C9gLbU"
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot , predicted_ids = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "  print('predicted_ids : ', predicted_ids)\n",
        "\n",
        "  # attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  # plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
        "\n",
        "  return predicted_ids\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEqzhYJf0UTT"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkx4Gn4RgQ0a"
      },
      "source": [
        "translate('forest fire near la ronge canada')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpOUJk0lAeO_"
      },
      "source": [
        "# create  results csv\n",
        "\n",
        "pred_ds = pd.read_csv('test.csv')\n",
        "\n",
        "pred_ds['location'].fillna(value='unk', inplace=True)\n",
        "pred_ds['keyword'].fillna(value='k_unk', inplace=True)\n",
        "pred_ds = pred_ds.fillna('0')\n",
        "\n",
        "pred_ds[\"text\"] = pred_ds['text'] + pred_ds['keyword'] + pred_ds['location']\n",
        "\n",
        "pred_ds['text'] = pred_ds['text'].apply(lambda x: text_clean(x))\n",
        "\n",
        "results = []\n",
        "for pred_text in pred_ds['text']:\n",
        "  results.append( 1 if translate(pred_text)[0] == 'positive' else 0)\n",
        "\n",
        "test_results = pd.DataFrame(\n",
        "    {'id': pred_ds['id'], 'target': results})\n",
        "test_results.to_csv('results_transformer1.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS8RFH02j4Fs"
      },
      "source": [
        "test_results = pd.DataFrame(\n",
        "    {'id': pred_ds['id'], 'target': results})\n",
        "test_results.to_csv('results_transformer7.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiVdsp-HFXU0"
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "  \n",
        "stop_words = set(stopwords.words('english')) \n",
        "# stop_words = []\n",
        "word_tokens = word_tokenize(example_sent) \n",
        "  \n",
        "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "print(filtered_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imGauDM-wHOB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}